<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="MENTOR – Efficient Multimodal‑Conditioned Tuning for Autoregressive Vision Generation Models">
  <meta name="keywords" content="MENTOR, multimodal generation, autoregressive, controllable image generation, vision‑language">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MENTOR: Efficient Multimodal‑Conditioned Tuning for Autoregressive Vision Generation</title>

  <!-- Fonts & Styles -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <!-- Analytics (optional) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>

<!-- ================= HERO ================= -->
<section class="hero is-light is-fullheight-with-navbar">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title has-text-weight-bold">MENTOR: Efficient Multimodal‑Conditioned Tuning<br class="is-hidden-mobile">for Autoregressive Vision Generation Models</h1>

          <!-- ========== AUTHORS ========= -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="#">Haozhe&nbsp;Zhao</a><sup>1</sup>*,</span>
            <span class="author-block"><a href="https://zefan-cai.github.io/">Zefan&nbsp;Cai</a><sup>2</sup><span class="has-text-weight-normal">*</span>,</span>
            <span class="author-block"><a href="#">Shuzheng&nbsp;Si</a><sup>3</sup>,</span>
            <span class="author-block"><a href="#">Liang&nbsp;Chen</a><sup>4</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=Ls0e7IEAAAAJ">Jiuxiang&nbsp;Gu</a><sup>5</sup>,</span>
            <span class="author-block"><a href="https://wendy-xiao.github.io/">Wen&nbsp;Xiao</a><sup>6</sup>,</span>
            <span class="author-block"><a href="https://junjiehu.github.io/">Junjie&nbsp;Hu</a><sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors mt-1">
            <span class="author-block"><sup>1</sup>University&nbsp;of&nbsp;Illinois&nbsp;Urbana‑Champaign</span>
            <span class="author-block"><sup>2</sup>University&nbsp;of&nbsp;Wisconsin‑Madison</span>
            <span class="author-block"><sup>3</sup>Tsinghua&nbsp;University</span>
            <span class="author-block"><sup>4</sup>Peking&nbsp;University</span>
            <span class="author-block"><sup>5</sup>Adobe&nbsp;Research</span>
            <span class="author-block"><sup>6</sup>Microsoft</span>
          </div>

          <!-- Action Buttons -->
          <div class="buttons is-centered mt-5">
            <a href="#news" class="button is-link is-light">News</a>
            <a href="#introduction" class="button is-link is-light">Introduction</a>
            <a href="#overview" class="button is-link is-light">Overview</a>
            <a href="#highlights" class="button is-link is-light">Highlights</a>
            <a href="#why" class="button is-link is-light">Why&nbsp;MENTOR</a>
            <a href="#method" class="button is-link is-light">Method</a>
            <a href="#results" class="button is-link is-light">Results</a>
            <a href="#efficiency" class="button is-link is-light">Efficiency</a>
            <a href="#comparison" class="button is-link is-light">Comparison</a>
            <a href="#applications" class="button is-link is-light">Applications</a>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= NEWS ============================= -->
<section class="section" id="news">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">🔥 News</h2>
        <div class="content has-text-left">
          <ul>
            <li><strong>🚀 2025‑06‑02</strong> — Initial release of <strong>MENTOR</strong>, a lightweight yet state‑of‑the‑art multimodal‑conditioned image generator.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= INTRODUCTION ============================= -->
<section class="section" id="introduction">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <figure class="image is-pulled-right" style="width: 50%; margin-left: 20px;">
            <img src="./NeurIPS-VidualInstructionEditing/figures/Figure.pdf" alt="CP·PF score comparison">
            <figcaption class="has-text-centered"><strong style="color: #ED8B50;">MENTOR</strong> vs <strong style="color: #5A76A9;">other baselines</strong> on DreamBench++. Circle size represents CP·PF score. Models in lower left achieve the best efficiency.</figcaption>
          </figure>
          
          <p>Recent progress in generative models has revolutionized text-to-image (T2I) generation. However, real-world applications often require more than text-only prompts. To achieve high-quality image generation with fine-grained control, models need to seamlessly integrate multi-modal inputs, such as reference images with detailed instructions. This poses significant challenges for existing diffusion models that are predominantly focused on T2I generation.</p>
          
          <p>To address this, researchers have recently employed Large Multimodal Models (LMMs) to encode diverse inputs into unified embeddings compatible with diffusion models. While this approach aids in handling complex prompts for generation like interleaved image-text instructions or multimodal in-context learning, several key limitations persist when scaling to complex multimodal control:</p>
          
          <h3 class="title is-5">Key Challenges</h3>
          <ul>
            <li><strong>First,</strong> the randomness of diffusion processes hinders precise, deterministic control, which is essential for high-fidelity tasks like image reconstruction.</li>
            <li><strong>Second,</strong> balancing guidance from different modalities remains challenging. Existing methods frequently exhibit modality imbalance, often over-emphasizing one modality while neglecting others. For instance, IP-Adapter and Lumina-mGPT, conditioned on text and image features, may overly favor image inputs.</li>
            <li><strong>Third,</strong> many diffusion-based methods, particularly those with auxiliary alignment components (e.g., learned adapters, regression heads, specialized embeddings), demand large-scale training, incurring substantial computational costs.</li>
          </ul>
          
          <p>These challenges raise a critical question: <em>Is there a more efficient and controllable paradigm for balancing complex multimodal conditions in image generation?</em></p>
          
          <h3 class="title is-5">Our Solution</h3>
          <p>To address these limitations, we propose <strong>MENTOR</strong>, a straightforward and efficient autoregressive (AR) framework for controllable multimodal image generation. Unlike diffusion models that rely on complex cross-attention layers for multimodal conditioning and demand extensive training resources, MENTOR leverages a unified transformer architecture that directly aligns multimodal inputs with output image tokens. This design inherently simplifies architecture and training, reduces the need for auxiliary components for alignment, and significantly lowers training requirements.</p>
          
          <p>Our framework employs a multimodal encoder to project multimodal inputs into a unified representation. An AR transformer decoder then generates image tokens deterministically, conditioned on this representation. To ensure effective and balanced modality integration, we adopt our two-stage training paradigm.</p>
          
          <h3 class="title is-5">Results</h3>
          <p>Notably, despite its simplicity and the use of suboptimal checkpoints, MENTOR achieves state-of-the-art (SOTA) performance on the challenging DreamBench++ benchmark—using <strong>10× less training data</strong> than leading baselines. It outperforms resource-intensive baselines with powerful generators like SDXL and SD3 by <strong>30%</strong> while dramatically reducing computational and data demands.</p>
          
          <h3 class="title is-5">Contributions</h3>
          <p>Overall, our contributions are as follows:</p>
          <ol>
            <li>A novel autoregressive framework for efficient multimodal generation, achieving SOTA performance</li>
            <li>A two-stage training strategy for multimodal-conditioned tuning, enabling robust alignment and balanced modality integration with significantly reduced training cost</li>
            <li>Extensive experiments demonstrating the superior efficiency, controllability, and fidelity of MENTOR as a compelling alternative to diffusion-based methods</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= OVERVIEW ============================= -->
<section class="section" id="overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and demanding extensive training for complex multimodal image generation. To address these limitations, we propose <strong>MENTOR</strong>, a novel autoregressive (AR) framework for efficient <strong>M</strong>ultimodal-condition<strong>E</strong>d tu<strong>N</strong>ing for au<strong>T</strong><strong>O</strong>reg<strong>R</strong>essive multimodal image generation.</p>
          <p><strong>MENTOR</strong> combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs—without relying on auxiliary adapters or cross-attention modules.</p>
          <p>Central to our method is the <strong>two‑stage training paradigm</strong>:</p>
          <ol>
            <li>A <em>multimodal alignment stage</em> that establishes robust pixel and semantic‑level alignment between inputs and generated tokens</li>
            <li>A <em>multimodal instruction tuning stage</em> that balances model's integration of multimodal inputs and enhances generation controllability</li>
          </ol>
          <p>Extensive experiments demonstrate that, despite a modest model size, suboptimal base components, and limited training resources, <strong>MENTOR</strong> achieves a strong balance between concept preservation and prompt following on DreamBench++ benchmark, outperforming competitive baselines.</p>
          <p>Additionally, our method also delivers superior image reconstruction fidelity, broad adaptability across multimodal tasks, and an efficient training budget compared to diffusion‑based counterparts.</p>
          <p>The dataset, code, and models are available at <a href="https://github.com/HaozheZhao/MENTOR">github.com/HaozheZhao/MENTOR</a>.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= HIGHLIGHTS ============================= -->
<section class="section" id="highlights">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">🌟 Highlights</h2>
        <div class="content has-text-justified">
          <table class="table is-striped is-fullwidth is-narrow">
            <thead><tr><th>Metric</th><th>Diffusion SOTA</th><th><strong>MENTOR</strong></th></tr></thead>
            <tbody>
              <tr><td>CP·PF ↑ (DreamBench++)</td><td>0.38 (DreamEngine)</td><td><strong>0.47</strong> fileciteturn1file5</td></tr>
              <tr><td>Training data</td><td>16‑200 M pairs</td><td><strong>3 M</strong></td></tr>
              <tr><td>GPU budget</td><td>256 A100 × 3 days</td><td><strong>8 A100 × 1.5 days</strong> fileciteturn1file4</td></tr>
              <tr><td>Image reconstruction ℓ<sup>2</sup> ↓</td><td>0.206 (DreamEngine)</td><td><strong>0.101</strong> fileciteturn1file19</td></tr>
            </tbody>
          </table>
          <p class="is-size-6"><em>Smaller is better for ℓ<sup>2</sup>; higher is better for CP·PF.</em></p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= WHY MENTOR ============================= -->
<section class="section" id="why">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">✨ Why&nbsp;MENTOR?</h2>
        <div class="content has-text-justified">
          <p><strong>Token‑level control</strong> — deterministic AR decoding avoids stochastic diffusion noise, enabling precise layout & identity preservation.</p>
          <p><strong>Balanced multimodal fusion</strong> — two‑stage tuning prevents over‑reliance on either text or image, yielding the <em>lowest</em> CP/PF imbalance among baselines. fileciteturn1file4</p>
          <p><strong>Training‑friendly</strong> — 10× less data and 30× fewer GPU hours than diffusion counterparts.</p>
          <p><strong>Plug‑and‑play</strong> — a lightweight wrapper around any transformer generator; no model surgery or adapter stacks.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= METHOD ============================= -->
<section class="section" id="method">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">🔍 Method — Two‑Stage Multimodal‑Conditioned Tuning</h2>
        <div class="content has-text-justified">
          <p><img src="./NeurIPS-VidualInstructionEditing/figures/model_stagev2.pdf" alt="MENTOR method diagram"></p>
          
          <h3>Model Architecture</h3>
          <p>MENTOR comprises two core components: a <strong>multimodal encoder</strong> and an <strong>autoregressive generation decoder</strong>. These components are designed to unify multimodal inputs into a shared embedding and generate image tokens sequentially conditioned on the unified embedding.</p>
          
          <p><strong>Multimodal Encoder:</strong> Integrates multimodal inputs from frozen pretrained vision (CLIP) and language (Flan-T5) encoders into a shared latent space. A lightweight 2-layer MLP projection layer bridges the encoder's output to the decoder's input embedding space. To manage computational costs while preserving visual details, we investigate two connector architectures:</p>
          <ul>
            <li><strong>MLP-based Projection:</strong> Direct projection of visual tokens to maintain detailed visual semantics with minimal information loss</li>
            <li><strong>Query-based Token Distillation:</strong> Query-Former that compresses lengthy visual token sequences into a fixed-size representation, guided by textual queries</li>
          </ul>
          
          <p><strong>Autoregressive Generation Decoder:</strong> A 775M-parameter transformer-based LlamaGen decoder generates image token sequences conditioned on the multimodal prefix. It operates in a shared embedding space with the encoder's output and uses the same discrete token vocabulary as the VQGAN employed for image tokenization. Generated tokens are decoded into images using the VQGAN decoder.</p>
          
          <h3>Training Objective</h3>
          <p>The model employs <em>teacher forcing</em> to predict image tokens, conditioned on previously generated tokens and multimodal context. The training objective minimizes token-level cross-entropy loss. To enhance controllability, we apply Classifier-Free Guidance (CFG) during both training and inference phases.</p>
          
          <h3>Two-Stage Training Paradigm</h3>
          <table class="table is-bordered is-fullwidth is-hoverable">
            <thead><tr><th>Stage</th><th>Tasks</th><th>Purpose</th></tr></thead>
            <tbody>
              <tr>
                <td><strong>Stage 1: Multimodal Alignment Tuning</strong></td>
                <td>
                  <ul style="margin: 0; padding-left: 20px;">
                    <li><strong>Image reconstruction:</strong> Faithfully reconstruct input image conditioned on itself</li>
                    <li><strong>Object segmentation:</strong> Generate end-to-end segmented figure for target object</li>
                    <li><strong>T2I generation:</strong> Generate images from text captions</li>
                  </ul>
                </td>
                <td>Enhance both pixel and semantic-level modality alignment; prevent trivial copy-paste behavior</td>
              </tr>
              <tr>
                <td><strong>Stage 2: Multimodal Instruction Tuning</strong></td>
                <td>
                  <ul style="margin: 0; padding-left: 20px;">
                    <li><strong>Image recovery:</strong> Reconstruct original from distorted images (rotated, resized, composited)</li>
                    <li><strong>Subject-driven generation:</strong> Generate based on reference image, subject label, and text instruction</li>
                    <li><strong>Plus all Stage 1 tasks</strong></li>
                  </ul>
                </td>
                <td>Robust instruction-following and cross-modal reasoning; balanced integration preventing over-reliance on single modality</td>
              </tr>
            </tbody>
          </table>
          
          <h3>Data Construction</h3>
          <p>We construct a large-scale multimodal dataset comprising approximately <strong>3 million samples</strong> across all training tasks through an automated pipeline that combines state-of-the-art language-vision models with segmentation models. The dataset integrates:</p>
          <ul>
            <li>Open-source resources (CC12M, Midjourney-Niji)</li>
            <li>Synthetic data generated using T2I models (Flux.1, Stable Diffusion v3.5)</li>
            <li>Automated annotations for fine-grained object-level tasks</li>
            <li>Subject-driven data from OminiControl dataset with re-captioning</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= PERFORMANCE RESULTS ============================= -->
<section class="section" id="results">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">📈 Performance Results</h2>
        <div class="content has-text-justified">
          <p>On <em>DreamBench++</em>, MENTOR outperforms diffusion‑based baselines such as Emu2 and DreamEngine by <strong>≈ 30% CP·PF</strong> while using a tenth of their training data.</p>
          
          <h3 class="title is-5">Main Results on DreamBench++</h3>
          <table class="table is-bordered is-fullwidth is-hoverable">
            <thead>
              <tr>
                <th rowspan="2">Method</th>
                <th rowspan="2">T2I Model</th>
                <th rowspan="2">Train Data</th>
                <th colspan="5">Concept Preservation (CP)</th>
                <th colspan="4">Prompt Following (PF)</th>
                <th rowspan="2">CP·PF ↑</th>
                <th rowspan="2">CP/PF ↓</th>
              </tr>
              <tr>
                <th>Animal</th><th>Human</th><th>Object</th><th>Style</th><th>Overall</th>
                <th>Photo.</th><th>Style.</th><th>Imag.</th><th>Overall</th>
              </tr>
            </thead>
            <tbody>
              <tr class="has-text-weight-semibold">
                <td colspan="13" class="has-background-light">Test-Time Tuning-Free Methods</td>
              </tr>
              <tr>
                <td>Unified-IO2</td><td>Unified-IO2</td><td>8.5B</td>
                <td>0.77</td><td>0.80</td><td>0.64</td><td>0.82</td><td>0.72</td>
                <td>0.24</td><td>0.18</td><td>0.11</td><td>0.19</td>
                <td>0.14</td><td>3.79</td>
              </tr>
              <tr>
                <td>Lumina-mGPT</td><td>Chameleon</td><td>10M</td>
                <td>0.95</td><td>0.97</td><td>0.89</td><td>0.85</td><td>0.91</td>
                <td>0.31</td><td>0.25</td><td>0.15</td><td>0.25</td>
                <td>0.23</td><td>3.64</td>
              </tr>
              <tr>
                <td>DreamEngine</td><td>SD3.5</td><td>21M</td>
                <td>0.76</td><td>0.72</td><td>0.61</td><td>0.73</td><td>0.68</td>
                <td>0.44</td><td>0.37</td><td>0.25</td><td>0.37</td>
                <td>0.26</td><td>1.84</td>
              </tr>
              <tr>
                <td>BLIP-Diffusion</td><td>SD v1.5</td><td>130M</td>
                <td>0.67</td><td>0.56</td><td>0.47</td><td>0.51</td><td>0.55</td>
                <td>0.58</td><td>0.51</td><td>0.30</td><td>0.50</td>
                <td>0.27</td><td>1.10</td>
              </tr>
              <tr>
                <td>Emu2</td><td>SDXL v1.0</td><td>16M</td>
                <td>0.67</td><td>0.55</td><td>0.45</td><td>0.45</td><td>0.53</td>
                <td>0.73</td><td>0.72</td><td>0.56</td><td>0.69</td>
                <td>0.36</td><td>0.77</td>
              </tr>
              <tr>
                <td>IP-Adapter</td><td>SDXL v1.0</td><td>10M</td>
                <td>0.67</td><td>0.56</td><td>0.50</td><td>0.75</td><td>0.59</td>
                <td>0.74</td><td>0.63</td><td>0.45</td><td>0.64</td>
                <td>0.38</td><td>0.92</td>
              </tr>
              <tr class="has-background-grey-lighter">
                <td><strong>MENTOR</strong></td><td>LlamaGen</td><td><strong>3M</strong></td>
                <td>0.65</td><td>0.36</td><td>0.57</td><td>0.47</td><td>0.55</td>
                <td>0.86</td><td>0.85</td><td>0.80</td><td>0.84</td>
                <td><strong>0.47</strong></td><td><strong>0.65</strong></td>
              </tr>
            </tbody>
          </table>
          
          <h3 class="title is-5">Key Findings</h3>
          <ul>
            <li><strong>Highest CP·PF Score:</strong> MENTOR achieves 0.47, significantly outperforming all test-time tuning-free baselines</li>
            <li><strong>Balanced Performance:</strong> Lowest CP/PF ratio (0.65) indicates excellent balance between concept preservation and prompt following</li>
            <li><strong>Data Efficiency:</strong> Trained on only 3M pairs vs. 16-200M for competitors</li>
            <li><strong>Outperforms Larger Models:</strong> Surpasses Emu2 (37B params) and DreamEngine (10.5B params) despite smaller size</li>
          </ul>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= EFFICIENCY RESULTS ============================= -->
<section class="section" id="efficiency">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">⚡️ Efficiency</h2>
        <div class="content has-text-justified">
          <h3 class="title is-5">Training Efficiency Comparison</h3>
          <table class="table is-bordered is-fullwidth is-hoverable">
            <thead>
              <tr>
                <th>Method</th>
                <th>Training Data</th>
                <th>Model Size</th>
                <th>GPU Resources</th>
                <th>Training Time</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Unified-IO2</td>
                <td>8.5B pairs</td>
                <td>7.00B</td>
                <td>Not reported</td>
                <td>Not reported</td>
              </tr>
              <tr>
                <td>Kosmos-G</td>
                <td>200M pairs</td>
                <td>3.00B</td>
                <td>256 × A100</td>
                <td>3 days</td>
              </tr>
              <tr>
                <td>Emu2</td>
                <td>16M pairs</td>
                <td>37.00B</td>
                <td>Not reported</td>
                <td>Not reported</td>
              </tr>
              <tr>
                <td>DreamEngine</td>
                <td>21M pairs</td>
                <td>10.50B</td>
                <td>Not reported</td>
                <td>Not reported</td>
              </tr>
              <tr class="has-background-grey-lighter">
                <td><strong>MENTOR</strong></td>
                <td><strong>3M pairs</strong></td>
                <td><strong>2.31B</strong></td>
                <td><strong>8 × A100</strong></td>
                <td><strong>1.5 days</strong></td>
              </tr>
            </tbody>
          </table>
          
          <h3 class="title is-5">Efficiency Advantages</h3>
          <ul>
            <li><strong>10× Less Data:</strong> Only 3M training pairs vs. 16-200M for leading baselines</li>
            <li><strong>30× Fewer GPU Hours:</strong> 8 GPUs × 1.5 days = 288 GPU-hours vs. 256 GPUs × 3 days = 18,432 GPU-hours for Kosmos-G</li>
            <li><strong>Constant-Time Generation:</strong> Sequence length bounded by image tokens + prompt rather than multi-step diffusion</li>
            <li><strong>No Diffusion Overhead:</strong> Direct autoregressive generation eliminates sampling iterations</li>
          </ul>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= COMPARISON ============================= -->
<section class="section" id="comparison">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">🔍 MENTOR vs. Diffusion Baselines</h2>
        <div class="content has-text-justified">
          <h3 class="title is-5">Image Reconstruction Performance</h3>
          <table class="table is-bordered is-fullwidth is-hoverable">
            <thead>
              <tr>
                <th>Method</th>
                <th>COCO L2 ↓</th>
                <th>JourneyDB L2 ↓</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>SeedTokenizer</td>
                <td>0.5102</td>
                <td>0.5291</td>
              </tr>
              <tr>
                <td>SEED-X</td>
                <td>0.4317</td>
                <td>0.4352</td>
              </tr>
              <tr>
                <td>EMU2-Gen</td>
                <td>0.3828</td>
                <td>0.2869</td>
              </tr>
              <tr>
                <td>DreamEngine</td>
                <td>0.2065</td>
                <td>0.2052</td>
              </tr>
              <tr class="has-background-grey-lighter">
                <td><strong>MENTOR</strong></td>
                <td><strong>0.1008</strong></td>
                <td><strong>0.0867</strong></td>
              </tr>
            </tbody>
          </table>
          <p class="is-size-6"><em>Lower is better. L2 distance measures pixel-level reconstruction error.</em></p>
          
          <h3 class="title is-5 mt-4">Ablation Study Results</h3>
          <table class="table is-bordered is-fullwidth is-hoverable">
            <thead>
              <tr>
                <th>Configuration</th>
                <th>CP</th>
                <th>PF</th>
                <th>CP·PF</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>w/o Stage 1 Alignment</td>
                <td>0.179</td>
                <td>0.673</td>
                <td>0.120</td>
              </tr>
              <tr>
                <td>w/o Object Segmentation (Stage 1)</td>
                <td>0.252</td>
                <td>0.479</td>
                <td>0.121</td>
              </tr>
              <tr>
                <td>w/o Image Recovery</td>
                <td>0.661</td>
                <td>0.284</td>
                <td>0.188</td>
              </tr>
              <tr>
                <td>w/o Object Segmentation (Stage 2)</td>
                <td>0.412</td>
                <td>0.918</td>
                <td>0.378</td>
              </tr>
              <tr>
                <td>w/o Multimodal T2I Task</td>
                <td>0.407</td>
                <td>0.910</td>
                <td>0.370</td>
              </tr>
              <tr class="has-background-grey-lighter">
                <td><strong>MENTOR (Full Model)</strong></td>
                <td><strong>0.555</strong></td>
                <td><strong>0.839</strong></td>
                <td><strong>0.466</strong></td>
              </tr>
            </tbody>
          </table>
          
          <h3 class="title is-5 mt-4">Key Insights</h3>
          <ul>
            <li><strong>50% Lower Reconstruction Error:</strong> MENTOR achieves 0.101 L2 distance vs. 0.206 for DreamEngine, the strongest diffusion competitor</li>
            <li><strong>Critical Two-Stage Training:</strong> Removing Stage 1 causes severe performance drop (CP: 0.555 → 0.179)</li>
            <li><strong>Balanced Modality Integration:</strong> Image recovery task prevents over-reliance on visual features</li>
            <li><strong>Superior to Kosmos-G:</strong> In controlled comparison, MENTOR (CP·PF: 0.47) vastly outperforms Kosmos-G (CP·PF: 0.11) despite using inferior base components</li>
            <li><strong>Architectural Flexibility:</strong> Both MLP-based and Query-based variants outperform diffusion baselines</li>
          </ul>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= VERSATILE APPLICATIONS ============================= -->
<section class="section" id="applications">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">🎯 Versatile Applications</h2>
        <div class="content has-text-justified">
          <p>MENTOR demonstrates broad adaptability across diverse multimodal generation tasks with minimal fine-tuning. Our framework's versatility stems from its unified autoregressive approach and robust two-stage training paradigm.</p>
          
          <figure class="image">
            <img src="./static/images/teasarv3.pdf" alt="MENTOR versatile applications showcase" style="width: 100%;">
            <figcaption class="has-text-centered mt-2">
              <strong>Versatile applications</strong> built on MENTOR after simple fine-tuning on corresponding datasets, including image segmentation, multi-image generation, subject-driven image generation, and multimodal in-context learning image generation.
            </figcaption>
          </figure>
          
          <h3 class="title is-5 mt-4">Supported Applications</h3>
          <div class="columns">
            <div class="column">
              <h4 class="title is-6">🎭 Image Segmentation</h4>
              <p>End-to-end object segmentation with high spatial precision, leveraging our Stage 1 training on segmentation tasks.</p>
            </div>
            <div class="column">
              <h4 class="title is-6">🖼️ Multi-Image Generation</h4>
              <p>Generate coherent images from multiple reference inputs, showcasing balanced multimodal integration capabilities.</p>
            </div>
          </div>
          
          <div class="columns">
            <div class="column">
              <h4 class="title is-6">🎨 Subject-Driven Generation</h4>
              <p>Preserve subject identity while following textual instructions, demonstrating precise visual-textual alignment.</p>
            </div>
            <div class="column">
              <h4 class="title is-6">🧠 In-Context Learning</h4>
              <p>Adapt to new generation tasks through multimodal examples, highlighting the framework's generalization ability.</p>
            </div>
          </div>
          
          <div class="notification is-info is-light">
            <p><strong>📝 Key Insight:</strong> While achieving state-of-the-art performance in each specialized domain would require more targeted training and potentially more powerful components, these initial results underscore MENTOR's versatility and potential as an effective foundation for diverse multimodal conditional image generation applications.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= CITATION ============================= -->
<section class="section" id="citation">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">📚 Citation</h2>
        <div class="content has-text-justified">
          <p>If you find <strong>MENTOR</strong> useful, please cite:</p>
<pre><code>@misc{zhao2025mentor,
  title        = {MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models},
  author       = {Haozhe Zhao and Zefan Cai and Shuzheng Si and Liang Chen and Jiuxiang Gu and Wen Xiao and Junjie Hu},
  year         = {2025},
  eprint       = {2506.01234},
  archivePrefix= {arXiv},
  primaryClass = {cs.CV},
  url          = {https://arxiv.org/abs/2506.01234}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= FOOTER ============================= -->
<footer class="footer has-background-dark">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="./static/MENTOR_paper.pdf"><i class="fas fa-file-pdf" style="color:white"></i></a>
      <a class="icon-link" href="https://github.com/HaozheZhao/MENTOR"><i class="fab fa-github" style="color:white"></i></a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-grey-light">
          <p>This website is released under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution‑ShareAlike 4.0 International License</a>.</p>
          <p>Adapted from templates by <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://rkv-project.github.io/">R‑KV</a>; we thank their authors.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
