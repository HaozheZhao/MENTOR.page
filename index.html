<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="MENTOR â€“ Efficient Multimodalâ€‘Conditioned Tuning for Autoregressive Vision Generation Models">
  <meta name="keywords" content="MENTOR, multimodal generation, autoregressive, controllable image generation, visionâ€‘language">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MENTOR: Efficient Multimodalâ€‘Conditioned Tuning for Autoregressive Vision Generation</title>

  <!-- Fonts & Styles -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <!-- Analytics (optional) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>

<!-- ================= HERO ================= -->
<section class="hero is-light is-fullheight-with-navbar">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title has-text-weight-bold">MENTOR: Efficient Multimodalâ€‘Conditioned Tuning<br class="is-hidden-mobile">for Autoregressive Vision Generation Models</h1>

          <!-- ========== AUTHORS ========= -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://haozhezhao.github.io/">Haozhe&nbsp;Zhao</a><sup>1</sup>*,</span>
            <span class="author-block"><a href="https://zefan-cai.github.io/">Zefan&nbsp;Cai</a><sup>2</sup><span class="has-text-weight-normal">*</span>,</span>
            <span class="author-block"><a href="https://s1s-z.github.io/">Shuzheng&nbsp;Si</a><sup>3</sup>,</span>
            <span class="author-block"><a href="https://chenllliang.github.io/">Liang&nbsp;Chen</a><sup>4</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=Ls0e7IEAAAAJ">Jiuxiang&nbsp;Gu</a><sup>5</sup>,</span>
            <span class="author-block"><a href="https://wendy-xiao.github.io/">Wen&nbsp;Xiao</a><sup>6</sup>,</span>
            <span class="author-block"><a href="https://junjiehu.github.io/">Junjie&nbsp;Hu</a><sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors mt-1">
            <span class="author-block"><sup>1</sup>University&nbsp;of&nbsp;Illinois&nbsp;Urbanaâ€‘Champaign</span>
            <span class="author-block"><sup>2</sup>University&nbsp;of&nbsp;Wisconsinâ€‘Madison</span>
            <span class="author-block"><sup>3</sup>Tsinghua&nbsp;University</span>
            <span class="author-block"><sup>4</sup>Peking&nbsp;University</span>
            <span class="author-block"><sup>5</sup>Adobe&nbsp;Research</span>
            <span class="author-block"><sup>6</sup>Microsoft</span>
          </div>

          <!-- Action Buttons -->
          <div class="buttons is-centered mt-5">
            <a href="#news" class="button is-link is-light">News</a>
            <a href="#overview" class="button is-link is-light">Overview</a>
            <a href="#highlights" class="button is-link is-light">Highlights</a>
            <a href="#why" class="button is-link is-light">Why&nbsp;MENTOR</a>
            <a href="#method" class="button is-link is-light">Method</a>
            <a href="#results" class="button is-link is-light">Results</a>
            <a href="#applications" class="button is-link is-light">Applications</a>
          </div>

          <div class="buttons is-centered mt-3">
  <a href="https://huggingface.co/datasets/BleachNick/Mentor_Stage1" class="button is-info is-light">
    ğŸ“¦ Dataset
  </a>
  <a href="https://huggingface.co/BleachNick/Mentor" class="button is-info is-light">
    ğŸ§  Model
  </a>
  <a href="https://github.com/HaozheZhao/MENTOR" class="button is-dark is-light">
    <i class="fab fa-github mr-1"></i> GitHub
  </a>
  <a href="https://arxiv.org/abs/" class="button is-danger is-light">
    ğŸ“„ arXiv
  </a>
</div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= NEWS ============================= -->
<section class="section" id="news">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ğŸ”¥ News</h2>
        <div class="content has-text-left">
          <ul>
            <li><strong>ğŸš€Â 2025â€‘06â€‘02</strong> â€” Initial release of <strong>MENTOR</strong>, a lightweight yet stateâ€‘ofâ€‘theâ€‘art multimodalâ€‘conditioned image generator.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= OVERVIEW ============================= -->
<section class="section" id="overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ğŸ§  Overview</h2>
        <div class="content has-text-justified">

          <figure class="image is-pulled-right" style="width: 50%; margin-left: 20px;">
            <img src="./figures/Figure.png" alt="CPÂ·PF score comparison">
            <figcaption class="has-text-centered">
              <strong style="color: #ED8B50;">MENTOR</strong> vs <strong style="color: #5A76A9;">baselines</strong> on DreamBench++. Circle size = CPÂ·PF score.
            </figcaption>
          </figure>

          <p><strong>MENTOR</strong>, a lightweight autoregressive (AR) framework for controllable multimodal image generation. Unlike diffusion models that rely on stochastic sampling and heavy training, MENTOR leverages a unified transformer to directly align multimodal inputs with output image tokens, enabling precise, token-level control with significantly fewer resources.</p>

          <p>Our <strong>two-stage training</strong> ensures balanced fusion across modalities and avoids over-reliance on either text or visual cues.</p>

          <ul>
            <li>ğŸš€ 10Ã— less data & fewer GPU hours than diffusion-based models</li>
            <li>ğŸ“ˆ SOTA on DreamBench++ with CPÂ·PF score of <strong>0.47</strong></li>
            <li>ğŸ¯ Effective for diverse downstream tasks: image reconstruction, subject-driven generation, multimodal ICL</li>
          </ul>

          <p>Code, data, and models available at:
            <a href="https://github.com/HaozheZhao/MENTOR">github.com/HaozheZhao/MENTOR</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= HIGHLIGHTS ============================= -->
<section class="section" id="highlights">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ğŸŒŸ Highlights</h2>
        <div class="content has-text-justified">
          <table class="table is-striped is-fullwidth is-narrow">
            <thead><tr><th>Metric</th><th>DiffusionÂ SOTA</th><th><strong>MENTOR</strong></th></tr></thead>
            <tbody>
              <tr><td>CPÂ·PFÂ â†‘ (DreamBench++)</td><td>0.36Â (Emu2)</td><td><strong>0.47</strong></td></tr>
              <tr><td>TrainingÂ data</td><td>16â€‘200â€¯M pairs</td><td><strong>3â€¯M</strong></td></tr>
              <tr><td>GPUÂ budget</td><td>256Â GPUÂ Ã—Â 3Â days(Kosmos-G)</td><td><strong>8Â GPUÂ Ã—Â 1.5Â days</strong></td></tr>
              <tr><td>Image reconstructionÂ â„“<sup>2</sup>Â â†“</td><td>0.206Â (DreamEngine)</td><td><strong>0.101</strong></td></tr>
            </tbody>
          </table>
          <p class="is-size-6"><em>Smaller is better for â„“<sup>2</sup>; higher is better for CPÂ·PF.</em></p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= WHY MENTOR ============================= -->
<section class="section" id="why">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">âœ¨ Why&nbsp;MENTOR?</h2>
        <div class="content has-text-justified">
          <p><strong>Tokenâ€‘level control</strong> â€” deterministic AR decoding avoids stochastic diffusion noise, enabling precise layout & identity preservation.</p>
          <p><strong>Balanced multimodal fusion</strong> â€” twoâ€‘stage tuning prevents overâ€‘reliance on either text or image, yielding the <em>lowest</em> CP/PF imbalance among baselines. îˆ€fileciteîˆ‚turn1file4îˆ</p>
          <p><strong>Trainingâ€‘friendly</strong> â€” 10Ã— less data and fewer GPU hours than diffusion counterparts.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= METHOD ============================= -->
<section class="section" id="method">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ğŸ” Method â€” Twoâ€‘Stage Multimodalâ€‘Conditioned Tuning</h2>
        <div class="content has-text-justified">
          <p><img src="./figures/model_stagev2.png" alt="MENTOR method diagram"></p>
          
          <h3>Two-Stage Training Paradigm</h3>
          <table class="table is-fullwidth is-hoverable is-bordered has-text-centered">
            <thead class="has-background-light">
              <tr>
                <th style="width: 20%;">Stage</th>
                <th style="width: 40%;">Tasks</th>
                <th style="width: 40%;">Objective</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Stage 1</strong><br><em>Alignment</em></td>
                <td>
                  <ul style="margin: 0; list-style-type: none; padding-left: 0;">
                    <li>â€¢ Image Reconstruction</li>
                    <li>â€¢ Object Segmentation</li>
                    <li>â€¢ Text-to-Image (T2I)</li>
                  </ul>
                </td>
                <td>Enhance pixel and semantic alignment</td>
              </tr>
              <tr>
                <td><strong>Stage 2</strong><br><em>Instruction Tuning</em></td>
                <td>
                  <ul style="margin: 0; list-style-type: none; padding-left: 0;">
                    <li>â€¢ Image Recovery</li>
                    <li>â€¢ Subject-Driven Generation</li>
                  </ul>
                </td>
                <td>Achieve robust and balanced multimodal integration</td>
              </tr>
            </tbody>
          </table>

          
          <h3>Data Construction</h3>
          <p><img src="./figures/data_construct.png" alt="MENTOR data_construct"></p>
          <p>We construct a large-scale multimodal dataset comprising approximately <strong>3 million samples</strong> across all training tasks through an automated pipeline that combines state-of-the-art language-vision models with segmentation models. The dataset integrates:</p>
          <ul>
            <li>Open-source resources (CC12M, Midjourney-Niji)</li>
            <li>Synthetic data generated using T2I models (Flux.1, Stable Diffusion v3.5)</li>
            <li>Automated annotations for fine-grained object-level tasks</li>
            <li>Subject-driven data from OminiControl dataset with re-captioning</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= PERFORMANCE RESULTS ============================= -->
<section class="section" id="results">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ğŸ“ˆ Performance Results</h2>
        <div class="content has-text-justified">
          <p>On <em>DreamBench++</em>, MENTOR outperforms diffusionâ€‘based baselines such as Emu2 and DreamEngine by <strong>â‰ˆ 30% CPÂ·PF</strong> while using a tenth of their training data.</p>
          
          <h3 class="title is-5">Main Results on DreamBench++</h3>
          <table class="table is-bordered is-fullwidth is-hoverable">
            <thead>
              <tr>
                <th rowspan="2">Method</th>
                <th rowspan="2">T2I Model</th>
                <th rowspan="2">Train Data</th>
                <th colspan="5">Concept Preservation (CP)</th>
                <th colspan="4">Prompt Following (PF)</th>
                <th rowspan="2">CPÂ·PF â†‘</th>
                <th rowspan="2">CP/PF â†“</th>
              </tr>
              <tr>
                <th>Animal</th><th>Human</th><th>Object</th><th>Style</th><th>Overall</th>
                <th>Photo.</th><th>Style.</th><th>Imag.</th><th>Overall</th>
              </tr>
            </thead>
            <tbody>
              <tr class="has-text-weight-semibold">
                <td colspan="13" class="has-background-light">Test-Time Tuning-Free Methods</td>
              </tr>
              <tr>
                <td>Unified-IO2</td><td>Unified-IO2</td><td>8.5B</td>
                <td>0.77</td><td>0.80</td><td>0.64</td><td>0.82</td><td>0.72</td>
                <td>0.24</td><td>0.18</td><td>0.11</td><td>0.19</td>
                <td>0.14</td><td>3.79</td>
              </tr>
              <tr>
                <td>Lumina-mGPT</td><td>Chameleon</td><td>10M</td>
                <td>0.95</td><td>0.97</td><td>0.89</td><td>0.85</td><td>0.91</td>
                <td>0.31</td><td>0.25</td><td>0.15</td><td>0.25</td>
                <td>0.23</td><td>3.64</td>
              </tr>
              <tr>
                <td>DreamEngine</td><td>SD3.5</td><td>21M</td>
                <td>0.76</td><td>0.72</td><td>0.61</td><td>0.73</td><td>0.68</td>
                <td>0.44</td><td>0.37</td><td>0.25</td><td>0.37</td>
                <td>0.26</td><td>1.84</td>
              </tr>
              <tr>
                <td>BLIP-Diffusion</td><td>SD v1.5</td><td>130M</td>
                <td>0.67</td><td>0.56</td><td>0.47</td><td>0.51</td><td>0.55</td>
                <td>0.58</td><td>0.51</td><td>0.30</td><td>0.50</td>
                <td>0.27</td><td>1.10</td>
              </tr>
              <tr>
                <td>Emu2</td><td>SDXL v1.0</td><td>16M</td>
                <td>0.67</td><td>0.55</td><td>0.45</td><td>0.45</td><td>0.53</td>
                <td>0.73</td><td>0.72</td><td>0.56</td><td>0.69</td>
                <td>0.36</td><td>0.77</td>
              </tr>
              <tr>
                <td>IP-Adapter</td><td>SDXL v1.0</td><td>10M</td>
                <td>0.67</td><td>0.56</td><td>0.50</td><td>0.75</td><td>0.59</td>
                <td>0.74</td><td>0.63</td><td>0.45</td><td>0.64</td>
                <td>0.38</td><td>0.92</td>
              </tr>
              <tr class="has-background-grey-lighter">
                <td><strong>MENTOR</strong></td><td>LlamaGen</td><td><strong>3M</strong></td>
                <td>0.65</td><td>0.36</td><td>0.57</td><td>0.47</td><td>0.55</td>
                <td>0.86</td><td>0.85</td><td>0.80</td><td>0.84</td>
                <td><strong>0.47</strong></td><td><strong>0.65</strong></td>
              </tr>
            </tbody>
          </table>
          
          <h3 class="title is-5">Key Findings</h3>
          <ul>
            <li><strong>Highest CPÂ·PF Score:</strong> MENTOR achieves 0.47, significantly outperforming all test-time tuning-free baselines</li>
            <li><strong>Balanced Performance:</strong> Lowest CP/PF ratio (0.65) indicates excellent balance between concept preservation and prompt following</li>
            <li><strong>Data Efficiency:</strong> Trained on only 3M pairs vs. 16-200M for competitors</li>
            <li><strong>Outperforms Larger Models:</strong> Surpasses Emu2 (37B params) and DreamEngine (10.5B params) despite smaller size</li>
          </ul>
        </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <h3 class="title is-5">Image Reconstruction Performance</h3>
          <table class="table is-bordered is-fullwidth is-hoverable">
            <thead>
              <tr>
                <th>Method</th>
                <th>COCO L2 â†“</th>
                <th>JourneyDB L2 â†“</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>SeedTokenizer</td>
                <td>0.5102</td>
                <td>0.5291</td>
              </tr>
              <tr>
                <td>SEED-X</td>
                <td>0.4317</td>
                <td>0.4352</td>
              </tr>
              <tr>
                <td>EMU2-Gen</td>
                <td>0.3828</td>
                <td>0.2869</td>
              </tr>
              <tr>
                <td>DreamEngine</td>
                <td>0.2065</td>
                <td>0.2052</td>
              </tr>
              <tr class="has-background-grey-lighter">
                <td><strong>MENTOR</strong></td>
                <td><strong>0.1008</strong></td>
                <td><strong>0.0867</strong></td>
              </tr>
            </tbody>
          </table>
          <p class="is-size-6"><em>Lower is better. L2 distance measures pixel-level reconstruction error.</em></p>
          
          <h3 class="title is-5 mt-4">Ablation Study Results</h3>
          <table class="table is-bordered is-fullwidth is-hoverable">
            <thead>
              <tr>
                <th>Configuration</th>
                <th>CP</th>
                <th>PF</th>
                <th>CPÂ·PF</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>w/o Stage 1 Alignment</td>
                <td>0.179</td>
                <td>0.673</td>
                <td>0.120</td>
              </tr>
              <tr>
                <td>w/o Object Segmentation (Stage 1)</td>
                <td>0.252</td>
                <td>0.479</td>
                <td>0.121</td>
              </tr>
              <tr>
                <td>w/o Image Recovery</td>
                <td>0.661</td>
                <td>0.284</td>
                <td>0.188</td>
              </tr>
              <tr>
                <td>w/o Object Segmentation (Stage 2)</td>
                <td>0.412</td>
                <td>0.918</td>
                <td>0.378</td>
              </tr>
              <tr>
                <td>w/o Multimodal T2I Task</td>
                <td>0.407</td>
                <td>0.910</td>
                <td>0.370</td>
              </tr>
              <tr class="has-background-grey-lighter">
                <td><strong>MENTOR (Full Model)</strong></td>
                <td>0.555</td>
                <td>0.839</td>
                <td><strong>0.466</strong></td>
              </tr>
            </tbody>
          </table>
          
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= VERSATILE APPLICATIONS ============================= -->
<section class="section" id="applications">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ğŸ¯ Versatile Applications</h2>
        <div class="content has-text-justified">
          <p>MENTOR demonstrates broad adaptability across diverse multimodal generation tasks with minimal fine-tuning. Our framework's versatility stems from its unified autoregressive approach and robust two-stage training paradigm.</p>
          
          <figure class="image">
            <img src="./figures/teasarv3.png" alt="MENTOR versatile applications showcase" style="width: 100%;">
            <figcaption class="has-text-centered mt-2">
              <strong>Versatile applications</strong> built on MENTOR after simple fine-tuning on corresponding datasets, including image segmentation, multi-image generation, subject-driven image generation, and multimodal in-context learning image generation.
            </figcaption>
          </figure>
          
   <h3 class="title is-5 mt-4">Supported Applications</h3>

    <div class="columns is-multiline is-centered">
      <div class="column is-half">
        <h4 class="title is-6">ğŸ­ Image Segmentation</h4>
        <figure class="image mb-2">
          <img src="./figures/segmentation_example.png" alt="Segmentation Example">
        </figure>
        <p>End-to-end object segmentation with high spatial precision, enabled by Stage 1 training.</p>
      </div>

      <div class="column is-half">
        <h4 class="title is-6">ğŸ–¼ï¸ Multi-Image Generation</h4>
        <figure class="image mb-2">
          <img src="./figures/multi_img.png" alt="Multi-Image Generation">
        </figure>
        <p>Generate coherent outputs from multiple reference images, demonstrating multimodal alignment.</p>
      </div>

      <div class="column is-half">
        <h4 class="title is-6">ğŸ§  In-Context Learning</h4>
        <figure class="image mb-2">
          <img src="./figures/icl_exp.png" alt="In-Context Learning Example">
        </figure>
        <p>Adapt to unseen generation tasks from few multimodal examples, showing strong generalization.</p>
      </div>
    </div>

          
          <div class="notification is-info is-light">
            <p><strong>ğŸ“ Key Insight:</strong> While achieving state-of-the-art performance in each specialized domain would require more targeted training and potentially more powerful components, these initial results underscore MENTOR's versatility and potential as an effective foundation for diverse multimodal conditional image generation applications.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= CITATION ============================= -->
<section class="section" id="citation">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ğŸ“š Citation</h2>
        <div class="content has-text-justified">
          <p>If you find <strong>MENTOR</strong> useful, please cite:</p>
<pre><code>@inproceedings{zhao2024mentor,
  title={MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models},
  author={Zhao, Haozhe* and Cai, Zefan* and Si, Shuzheng and Chen, Liang and 
          Gu, Jiuxiang and Xiao, Wen and Hu, Junjie},
  year={2024}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ============================= FOOTER ============================= -->
<footer class="footer has-background-dark">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="./figures/MENTOR_paper.png"><i class="fas fa-file-pdf" style="color:white"></i></a>
      <a class="icon-link" href="https://github.com/HaozheZhao/MENTOR"><i class="fab fa-github" style="color:white"></i></a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-grey-light">
          <p>This website is released under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attributionâ€‘ShareAlikeÂ 4.0 International License</a>.</p>
          <p>Adapted from templates by <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://rkv-project.github.io/">Râ€‘KV</a>; we thank their authors.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
