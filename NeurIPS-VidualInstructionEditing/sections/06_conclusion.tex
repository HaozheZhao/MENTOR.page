\section{Conclusion}
\label{sec:conclusion}

In this work, we introduced a controllable and efficient autoregressive framework for complex multimodal image generation, offering a compelling alternative to diffusion-based methods.
By unifying multimodal inputs within an AR model and leveraging a two-stage training paradigm, our method achieves state-of-the-art performance on challenging benchmarks—despite a modest model size, suboptimal base component, and limited training resources.
These results underscore efficiency, scalability, and controllability of our method, establishing it as a efficient foundation for building versatile, fine-grained visual generation systems capable of handling complex multimodal prompts.

% By unifying multimodal inputs within AR framework and employing a two-stage training paradigm, our framework achieves state-of-the-art performance on complex benchmarks, demonstrating its efficiency, scalability, and controllability, even with a modest model size and significantly reduced training resources. This work establishes autoregressive models as a practical and scalable solution for multimodal image generation, laying a solid foundation for building versatile and controllable systems capable of nuanced visual generation from complex prompts.




% In this work, we have introduced an efficient and controllable autoregressive framework for complex multimodal image generation, effectively addressing the limitations inherent to diffusion-based methods. Our approach leverages a unified autoregressive transformer architecture that integrates visual and textual inputs into a shared latent representation, enabling precise token-level control and alignment without auxiliary adapters or cross-attention blocks. Crucially, our novel two-stage training paradigm—comprising a multimodal alignment stage and a multimodal instruction tuning stage—ensures robust multimodal alignment and balanced modality conditioning, achieving substantial improvements in generation fidelity and instruction adherence.
% Extensive experiments demonstrate our framework's superior performance, surpassing state-of-the-art diffusion models despite employing significantly smaller architectures and reduced training resources. Our results highlight the effectiveness and scalability of autoregressive models in multimodal image generation tasks, establishing them as viable alternatives to diffusion-based counterparts. 
% Moving forward, our methodology offers a promising foundation for developing efficient, versatile, and controllable multimodal generative systems, paving the way for broader applications requiring precise and nuanced visual generation from complex multimodal prompts.