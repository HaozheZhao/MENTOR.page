\section{Related Work}
\label{sec:relatedwork}



% \begin{table}[htbp]
%   \centering
%   \setlength{\tabcolsep}{5pt}
%   \renewcommand{\arraystretch}{1.25}
%   \begin{tabular}{lccccccc}
%     \toprule
%     \textbf{Model} &
%     T$\Rightarrow$I &
%     I+T$\Rightarrow$T &
%     T+I$\Rightarrow$T+I &
%     Open-source &
%     Available &
%     \makecell[c]{Actual\\Support\\T+I$\Rightarrow$I} &
%     \makecell[c]{Within\\3 Month} \\
%     \midrule
%     EMU3              & \cmark & \cmark &        & \cmark & \cmark &        &        \\
%     LWM               & \cmark & \cmark &        & \cmark & \cmark & no train   &        \\
%     Unified-IO2       & \cmark & \cmark & ?      & \cmark & \cmark & \cmark &        \\
%     Lumina-mGPT       & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark &        \\
%     Lumina-mGPT 2.0   & \cmark & \cmark & \cmark & \cmark &        &  no train   & \cmark \\
%     Liquid            & \cmark & \cmark &        & \cmark & \cmark & \cmark &        \\
%     vargpt            & \cmark & \cmark & ?      & \cmark & \cmark & ?      &        \\
%     vargpt-1.1        & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
%     VILA-U            & \cmark &        & no train   & \cmark & \cmark & no train   &        \\
%     Janus             & \cmark & \cmark &        & \cmark & \cmark &        &        \\
%     MUSE-VL           & \cmark & \cmark &        &        &        &        &        \\
%     X-Prompt          & \cmark & \cmark & \cmark &        &        & \cmark &        \\
%     \bottomrule
%   \end{tabular}
%   \caption{Multimodal model capability and availability overview.}
%   \label{tab:model_overview}
% \end{table}



% \subsection{Image Generation with Complex Control}
% % blipdiffusion, kosmos-g, emu2, subject-diffusion, suti, control net
% Recent progress in controlled image generation using diffusion models has been significant. Researchers have explored various conditioning strategies—ranging from low-level cues like canny edges and depth maps~\citep{ye2023ip-adapter, controlnet} to higher-level guidance provided by reference images~\citep{SDEdit}—to steer the generative process. For instance, methods such as IP-Adapter~\citep{ye2023ip-adapter} and ControlNet~\citep{controlnet} incorporate additional control signals into standard text-to-image frameworks, thereby allowing more precise manipulation of generated content.
% In parallel, several works have leveraged visual elements from input images to further guide the generation process. DreamBooth~\citep{ruiz2023dreamboothfinetuningtexttoimage} and Textual Inversion~\citep{gal2022imageworthwordpersonalizing}, for example, adopt optimization-based approaches to adapt models to specific reference images. Although effective, these methods typically require extensive fine-tuning for each new input, limiting their practicality. To address these limitations, approaches like SuTI~\citep{suti} and Subject-diffusion~\citep{ma2024subjectdiffusionopendomainpersonalizedtexttoimage} have aimed to scale the fine-tuning process so that models can generalize across diverse reference images. However, these strategies still tend to be both time- and resource-intensive, highlighting the ongoing need for more efficient mechanisms for image generation with complex controls.

\subsection{Image Generation with Complex Multimodal Control}
Researchers have developed image generation via diffusion models conditioned on multimodal inputs like canny edges~\citep{controlnet} and reference images~\citep{ultraEdit,SDEdit}. ControlNet~\citep{controlnet} uses auxiliary parameters, while Mix-of-Show~\citep{Mix-of-Show} and FLUXSynID~\citep{FLUXSynID} use LoRA modules for multi-concept control and identity preservation.
DreamBooth~\citep{ruiz2023dreamboothfinetuningtexttoimage} enable subject-specific fine-tuning but limit generalization. SuTI~\citep{suti} address it with scalable data and training.
To enhance flexibility, recent work integrates LMMs with diffusion models by mapping LMM embedding into diffusion spaces~\citep{koh2023GILL,sun2023emu1,dreamllm,unimo}. Approaches like Kosmos-G~\citep{Kosmos-G}, Emu-2~\citep{emu2}, Seed-X~\citep{2024SeedX}, and DreamEngine~\citep{dreamengine} explore more complex multimodal prompt and fine-grained multimodal control. Yet, balancing guidance from diverse modalities remains a core challenge~\citep{han2024emmatexttoimagediffusionmodel,ye2023ip-adapter,RealCustom++}. 
EMMA~\citep{han2024emmatexttoimagediffusionmodel} employs a gated perceiver resampler for dynamic signal integration, while RealCustom++\citep{RealCustom++} disentangles subject identity and textual fidelity via cross-layer projectors. OmniControl~\citep{OminiControl} introduces a bias term into multimodal attention. 
Nonetheless, these method often require substantial computational resources, and achieving efficient, robust, and scalable multimodal integration remains an open problem.



\subsection{Autoregressive Multimodal Image Generation}
Autoregressive models have driven progress in T2I generation, from DALL·E~\cite{DALLE} and Parti~\citep{parti} to LlamaGen~\citep{llamagen} and GPT4O~\citep{gpt4o}.
Recent work extends it to multimodal settings: Models like Chameleon~\citep{chameleonteam2024chameleon}, LWM~\cite{LWM}, AnyGPT~\citep{Zhan2024AnyGPT}, and EMU3~\citep{Emu3} treat text and images as unified token sequences via early-fusion transformers, yet still emphasize text-to-image generation with limited support for multimodal conditioning.
Janus~\citep{Janus} decouples visual understanding and generation via distinct pathways, lacks support for multimodal image generation. MUSE-VL~\cite{musevl} and VILA-U~\citep{VILA-U}  align discrete visual tokens with text to improve perception, but remain oriented toward understanding tasks rather than image generation.
Unified-IO2~\citep{lu2023unifiedio2} is trained autoregressively from scratch for both understanding and generation across modalities, while Lumina-mGPT~\cite{2024lumina} enhances Chameleon with omnipotent supervised fine-tuning for broader multimodal tasks. Nonetheless, these models often over-rely on visual inputs while ignoring textual prompts.
Overall, while models like VILA-U~\cite{VILA-U}, EMU3~\cite{Emu3}, and Janus~\cite{Janus} have advanced text-to-image generation, robust multimodal conditional image generation remains an open and underexplored challenge.

